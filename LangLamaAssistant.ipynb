{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ],
      "metadata": {
        "id": "wWxUx7q3BXq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a679b33-349a-47d0-d112-43eeb10da4c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch"
      ],
      "metadata": {
        "id": "WwBFVh2FpJ5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "FvyQkNTSp5o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! huggingface-cli login"
      ],
      "metadata": {
        "id": "txPkPOBksnBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "import os"
      ],
      "metadata": {
        "id": "povLrfsSwA97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists(\"the-verdict.txt\"):\n",
        "  url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
        "  file_path = \"the-verdict.txt\"\n",
        "  urllib.request.urlretrieve(url, file_path)"
      ],
      "metadata": {
        "id": "DmnC2qVGylyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()"
      ],
      "metadata": {
        "id": "Rg6iQvQyyrrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = raw_text"
      ],
      "metadata": {
        "id": "2x9XZ1Fj0pOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_spliter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=50)\n",
        "chunk = text_spliter.split_text(document)"
      ],
      "metadata": {
        "id": "Lp7Vc-Zawnf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgMeq5O3xpyE",
        "outputId": "66165149-ad3a-4e97-f8f1-81f2b643187a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "116"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 Creating DB"
      ],
      "metadata": {
        "id": "J6iYNrhV1VBC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5b47c39"
      },
      "source": [
        "!pip install chromadb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = \"db\"\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "ATLn8TJL4T37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_texts(chunk, embedding, persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "OrdRCnWG4eRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriver = vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "du66Njqi6DrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "957eb437"
      },
      "source": [
        "!pip install llama-cpp-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c84d7421"
      },
      "source": [
        "## Install required libraries\n",
        "\n",
        "### Subtask:\n",
        "Install necessary libraries like `llama-cpp-python`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2951db9"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to install the `llama-cpp-python` library. The provided code cell already does this using `pip install`. I will execute this cell to complete the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c21f2910"
      },
      "source": [
        "!pip install llama-cpp-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2dd9663"
      },
      "source": [
        "## Download llama2 model\n",
        "\n",
        "### Subtask:\n",
        "Download a suitable Llama2 model file in a compatible format (e.g., GGML or GGUF).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c21edad"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to download a Llama2 model file in a compatible format (GGUF) to the current working directory. I will use `wget` for this purpose and download a specific Llama-2-7b-chat-hf GGUF model from Hugging Face.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e0ac610"
      },
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf -O llama-2-7b-chat.Q4_K_M.gguf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "550daf74"
      },
      "source": [
        "## Initialize llamacpp\n",
        "\n",
        "### Subtask:\n",
        "Use the downloaded model file to initialize the `LlamaCpp` class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa7a5113"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to initialize LlamaCpp failed due to a KeyError related to 'model_path'. The instructions explicitly state to set the `model_path` parameter. Therefore, I need to correct the initialization by providing the correct parameter name and the path to the downloaded model file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d776c100"
      },
      "source": [
        "llm = LlamaCpp(\n",
        "    model_path=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
        "    temperature=0.4,\n",
        "    max_tokens=512,\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=20,\n",
        "    verbose=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    chain_type=\"stuff\"\n",
        ")"
      ],
      "metadata": {
        "id": "DNxjYLO-BNdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Ask questions\n",
        "while True:\n",
        "    query = input(\"\\n🧠 Ask something about the document (or type 'exit'): \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    answer = qa_chain.run(query)\n",
        "    print(f\"\\n🤖 Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0oi7JHSBY5c",
        "outputId": "912d5026-6852-4a46-868f-73f7c9a01ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Ask something about the document (or type 'exit'): who is jack\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-82-3403371536.py:6: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  answer = qa_chain.run(query)\n",
            "Llama.generate: 1 prefix-match hit, remaining 288 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6295.90 ms\n",
            "llama_perf_context_print: prompt eval time =   89779.95 ms /   288 tokens (  311.74 ms per token,     3.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5974.77 ms /     9 runs   (  663.86 ms per token,     1.51 tokens per second)\n",
            "llama_perf_context_print:       total time =   95764.12 ms /   297 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🤖 Answer:  Jack is the protagonist of this passage.\n",
            "\n",
            "🧠 Ask something about the document (or type 'exit'): summarize the story\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 45 prefix-match hit, remaining 197 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6295.90 ms\n",
            "llama_perf_context_print: prompt eval time =   62611.51 ms /   197 tokens (  317.82 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =   15418.81 ms /    25 runs   (  616.75 ms per token,     1.62 tokens per second)\n",
            "llama_perf_context_print:       total time =   78055.00 ms /   222 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🤖 Answer:  I don't know.\n",
            "Unhelpful Answer: Jack gave up painting because he married his interest in interesting women.\n",
            "\n",
            "🧠 Ask something about the document (or type 'exit'): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f89f762"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a simple query and use the initialized llm object to get a response from the model.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}